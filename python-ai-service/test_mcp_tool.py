#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Enhanced test script for MCP tool integration with LLM
Tests both tool functionality and LLM's ability to detect and parameterize tool usage
"""
import os
import asyncio
import json
from pydantic import SecretStr
from langchain_openai import ChatOpenAI
from tools.practice_plan_tool import ngag_client


# Get configuration from environment variables
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL")
model_name = os.getenv("MODEL_NAME", "glm-4")

# Use default values if not provided
if not api_key:
    api_key = "you-are-key"
    print("Using default API key")

if not base_url:
    base_url = "https://open.bigmodel.cn/api/paas/v4"
    print(f"Using default base URL: {base_url}")

print(f"Model name: {model_name}")

# Initialize the LLM
llm = ChatOpenAI(
    model=model_name,
    api_key=SecretStr(api_key),
    base_url=base_url,
    temperature=0.7,
    max_completion_tokens=1000
)

async def test_tool_detection(llm, tools):
    """Test if LLM can correctly detect when to use tools and return proper parameters"""
    print("\n=== Testing Tool Detection ===")
    
    # Format tools information for the prompt
    tools_info = "\n".join([
        f"- {tool.name}: {tool.description}" 
        for tool in tools
    ])
    
    # Create detection prompt with explicit tool usage instructions
    detection_prompt = f"""
你是一位神经科学顾问，拥有以下工具可用:

{tools_info}

请分析用户的输入并确定：
1. 是否应该使用工具
2. 如果应该使用工具，选择哪个工具
3. 调用该工具的参数

用户输入: "请帮我生成一个7天正念减压修行计划的网页应用"
上下文信息: "用户想要一个正念修行的网页工具"

请以严格的JSON格式响应，包含以下字段:
- should_use_tool: boolean (是否使用工具)
- tool_name: string (工具名称，如果不使用工具则为null)
- tool_parameters: object (工具参数，如果不使用工具则为null)
- reasoning: string (你的推理过程)

只返回JSON格式的响应，不要有其他内容。

示例响应格式:
{{
    "should_use_tool": true,
    "tool_name": "generate_web_app",
    "tool_parameters": {{"title": "7天正念减压修行计划"}},
    "reasoning": "用户明确要求生成网页应用，generate_web_app工具专门用于此目的"
}}
    """
    
    print(f"Detection Prompt:\n{detection_prompt}")
    
    try:
        # Get LLM response
        response = await llm.ainvoke(detection_prompt)
        response_content = response.content.strip()
        
        print(f"\nLLM Response:\n{response_content}")
        
        # Parse JSON response
        try:
            decision = json.loads(response_content)
            
            # Validate response structure
            required_fields = ["should_use_tool", "tool_name", "tool_parameters", "reasoning"]
            for field in required_fields:
                if field not in decision:
                    raise ValueError(f"Missing required field: {field}")
            
            print(f"\nDecision: {json.dumps(decision, ensure_ascii=False, indent=2)}")
            return decision
            
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
            print(f"Raw response: {response_content}")
            return None
            
    except Exception as e:
        print(f"Error in tool detection: {e}")
        return None

async def test_tool_execution_with_llm_params(tools, tool_name, tool_parameters):
    """Test tool execution with parameters generated by LLM"""
    print(f"\n=== Testing Tool Execution with LLM Parameters ===")
    print(f"Tool: {tool_name}")
    print(f"Parameters: {json.dumps(tool_parameters, ensure_ascii=False, indent=2)}")
    
    # Find the tool
    tool = None
    for t in tools:
        if t.name == tool_name:
            tool = t
            break
    
    if not tool:
        print(f"Tool {tool_name} not found")
        return None
    
    try:
        # Execute the tool with LLM-generated parameters
        result = await tool.ainvoke(tool_parameters)
        print(f"Tool Result:\n{result}")
        return result
        
    except Exception as e:
        print(f"Error executing tool: {e}")
        return f"Error: {str(e)}"

async def test_response_generation(llm, tools, user_input, tool_result):
    """Test response generation after tool execution"""
    print("\n=== Testing Response Generation ===")
    
    # Format tools information
    tools_info = "\n".join([
        f"- {tool.name}: {tool.description}" 
        for tool in tools
    ])
    
    response_prompt = f"""
你是一位神经科学顾问，拥有以下工具可用:

{tools_info}

用户输入: "{user_input}"
工具执行结果: {tool_result}

请分析工具执行结果并给用户提供一个完整的响应，解释：
1. 工具生成了什么内容
2. 这个内容如何帮助用户达成目标
3. 接下来的建议步骤

请提供友好、专业的回应，让用户感觉被理解和帮助。

只返回最终的用户响应，不要有其他内容。
    """
    
    print(f"Response Generation Prompt:\n{response_prompt}")
    
    try:
        final_response = await llm.ainvoke(response_prompt)
        print(f"\nFinal Response:\n{final_response.content}")
        return final_response.content
        
    except Exception as e:
        print(f"Error generating final response: {e}")
        return None

async def test_mcp_tool():
    """Comprehensive test of MCP tool functionality with LLM integration"""
    print("=== Starting Comprehensive MCP Tool Test ===\n")
    
    # Get tools from the MCP server
    tools = await ngag_client.get_tools()
    
    print(f"Available tools: {[tool.name for tool in tools]}")
    
    # Test 1: Basic tool availability and functionality
    print("\n=== Test 1: Basic Tool Availability ===")
    generate_web_app_tool = None
    for tool in tools:
        if tool.name == "generate_web_app":
            generate_web_app_tool = tool
            break
    
    if generate_web_app_tool:
        print("Found generate_web_app tool")
        # Test the tool with a sample title
        result = await generate_web_app_tool.ainvoke({"title": "7天正念减压修行计划"})
        print("Generated HTML preview:")
        print(result[:500] + "..." if len(result) > 500 else result)
    else:
        print("generate_web_app tool not found")
        return
    
    # Test 2: LLM tool detection and parameter generation
    print("\n" + "="*60)
    user_input = "请帮我生成一个7天正念减压修行计划的网页应用"
    decision = await test_tool_detection(llm, tools)
    
    if decision and decision["should_use_tool"]:
        # Test 3: Tool execution with LLM-generated parameters
        tool_result = await test_tool_execution_with_llm_params(
            tools, decision["tool_name"], decision["tool_parameters"]
        )
        
        if tool_result:
            # Test 4: Response generation with tool result
            await test_response_generation(llm, tools, user_input, tool_result)
    else:
        print("LLM did not detect tool usage need")
    
    print("\n=== Test Complete ===")

if __name__ == "__main__":
    asyncio.run(test_mcp_tool())